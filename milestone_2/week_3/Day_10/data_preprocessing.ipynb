{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba690655",
   "metadata": {
    "id": "ba690655"
   },
   "source": [
    "### Data Preprocessing Pipeline for Image Segmentation\n",
    "- We are developing a simple data preprocessing pipeline using OpenCV and PyTorch for image segmentation tasks.\n",
    "- This pipeline includes loading an image and its corresponding mask, resizing, normalization, augmentation (random horizontal flip), and converting to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0VnBcvI5iWgm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4341,
     "status": "ok",
     "timestamp": 1764771999444,
     "user": {
      "displayName": "Ravi Chandr",
      "userId": "04022233043678125699"
     },
     "user_tz": -330
    },
    "id": "0VnBcvI5iWgm",
    "outputId": "0294aa0d-03b9-4a32-aed2-4cacbb0dca5e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6753a5f7",
   "metadata": {
    "id": "6753a5f7"
   },
   "source": [
    "#### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4acc46c0",
   "metadata": {
    "executionInfo": {
     "elapsed": 90,
     "status": "ok",
     "timestamp": 1764772004259,
     "user": {
      "displayName": "Ravi Chandr",
      "userId": "04022233043678125699"
     },
     "user_tz": -330
    },
    "id": "4acc46c0"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774affca",
   "metadata": {
    "id": "774affca"
   },
   "source": [
    "#### 2. Load image and mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "HR4yB6d1iJqQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "executionInfo": {
     "elapsed": 670,
     "status": "error",
     "timestamp": 1764772602682,
     "user": {
      "displayName": "Ravi Chandr",
      "userId": "04022233043678125699"
     },
     "user_tz": -330
    },
    "id": "HR4yB6d1iJqQ",
    "outputId": "67ca0135-f809-49e1-c614-54ccf20a584a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (480, 640, 3), Mask shape: (480, 640)\n"
     ]
    }
   ],
   "source": [
    "def load_image_and_mask(image_path, mask_path):\n",
    "    # Load image in BGR, convert to RGB\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Load mask in grayscale (assumed: 0 = background, >0 = subject)\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "# Example usage\n",
    "# image_path = \"/content/sample_data/california_housing_train.csv\"\n",
    "# mask_path  = \"/content/sample_data/mnist_train_small.csv\"\n",
    "image_path = r\"C:\\Users\\monal\\Desktop\\coco\\coco2017\\train2017\\000000000009.jpg\"\n",
    "mask_path  = r\"C:\\Users\\monal\\Desktop\\coco\\coco2017\\train2017\\000000000009.jpg\"\n",
    "\n",
    "image, mask = load_image_and_mask(image_path, mask_path)\n",
    "print(f\"Image shape: {image.shape}, Mask shape: {mask.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4181f76",
   "metadata": {
    "id": "d4181f76"
   },
   "source": [
    "- `cv2.imread(image_path)` loads the image in BGR format (OpenCV default).\n",
    "\n",
    "- `cv2.cvtColor(..., cv2.COLOR_BGR2RGB)` converts it to RGB so it matches the usual PyTorch convention.\n",
    "\n",
    "- `cv2.imread(..., cv2.IMREAD_GRAYSCALE)` loads the mask as a single-channel grayscale image (0 for background, non-zero for subject)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e11d7da",
   "metadata": {
    "id": "6e11d7da"
   },
   "source": [
    "#### 3. Resize image and mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "PiyWwA06iKxB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "error",
     "timestamp": 1764772606732,
     "user": {
      "displayName": "Ravi Chandr",
      "userId": "04022233043678125699"
     },
     "user_tz": -330
    },
    "id": "PiyWwA06iKxB",
    "outputId": "5ac5247f-e890-4b83-e4dc-ca25b7a0bb91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized image shape: (256, 256, 3), mask shape: (256, 256)\n"
     ]
    }
   ],
   "source": [
    "def resize_image_and_mask(image, mask, target_size=(256, 256)):\n",
    "    # Resize image using bilinear interpolation\n",
    "    image_resized = cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Resize mask using nearest neighbor (to avoid blurring class boundaries)\n",
    "    mask_resized = cv2.resize(mask, target_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    return image_resized, mask_resized\n",
    "\n",
    "# Example usage\n",
    "target_size = (256, 256)\n",
    "image_resized, mask_resized = resize_image_and_mask(image, mask, target_size)\n",
    "print(f\"Resized image shape: {image_resized.shape}, mask shape: {mask_resized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427c5594",
   "metadata": {
    "id": "427c5594"
   },
   "source": [
    "- `cv2.INTER_LINEAR` is good for images (smooth resizing).\n",
    "\n",
    "- `cv2.INTER_NEAREST` is used for masks to preserve sharp boundaries (no interpolation between classes).\n",
    "\n",
    "- `target_size` is usually a tuple like `(H, W)` or `(W, H)`; OpenCV uses `(width, height)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76052787",
   "metadata": {
    "id": "76052787"
   },
   "source": [
    "#### 4. Normalize image to and convert to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sipGkVQ3iL0p",
   "metadata": {
    "id": "sipGkVQ3iL0p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dtype: float32, range: [0.000, 1.000]\n"
     ]
    }
   ],
   "source": [
    "def normalize_image(image):\n",
    "    # Convert to float32 and scale to [0, 1]\n",
    "    image_float = image.astype(np.float32) / 255.0\n",
    "    return image_float\n",
    "\n",
    "# Example usage\n",
    "image_normalized = normalize_image(image_resized)\n",
    "print(f\"Image dtype: {image_normalized.dtype}, range: [{image_normalized.min():.3f}, {image_normalized.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06e4341",
   "metadata": {
    "id": "e06e4341"
   },
   "source": [
    "- Converts the image from uint8 (0–255) to float32 (0.0–1.0).\n",
    "\n",
    "- This is needed before applying PyTorch normalization transforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85061d",
   "metadata": {
    "id": "8e85061d"
   },
   "source": [
    "#### 5. Apply random horizontal flip (augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fpITZGg3iNbx",
   "metadata": {
    "id": "fpITZGg3iNbx"
   },
   "outputs": [],
   "source": [
    "def random_horizontal_flip(image, mask, p=0.5):\n",
    "    if np.random.rand() < p:\n",
    "        image = np.flip(image, axis=1)\n",
    "        mask  = np.flip(mask,  axis=1)\n",
    "    return image, mask\n",
    "\n",
    "# Example usage\n",
    "image_aug, mask_aug = random_horizontal_flip(image_normalized, mask_resized, p=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcb65b9",
   "metadata": {
    "id": "cfcb65b9"
   },
   "source": [
    "- Flips both image and mask horizontally with probability p.\n",
    "\n",
    "- This is a simple data augmentation that helps the model generalize better.\n",
    "\n",
    "- Always flip the mask in the same way as the image so the labels stay aligned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3db28a",
   "metadata": {
    "id": "5d3db28a"
   },
   "source": [
    "#### 6. Convert to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b3WuGBoiOZ4",
   "metadata": {
    "id": "3b3WuGBoiOZ4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tensor shape: torch.Size([3, 256, 256]), mask tensor shape: torch.Size([256, 256])\n"
     ]
    }
   ],
   "source": [
    "def to_tensor(image, mask):\n",
    "    # Ensure arrays are contiguous (np.flip can produce negative strides / non-contiguous views)\n",
    "    image = np.ascontiguousarray(image)\n",
    "    mask = np.ascontiguousarray(mask)\n",
    "\n",
    "    # Convert image: HWC → CHW and to tensor\n",
    "    image_tensor = torch.from_numpy(image).permute(2, 0, 1).float()  # HWC → CHW\n",
    "    mask_tensor  = torch.from_numpy(mask).long()                     # mask as long tensor\n",
    "\n",
    "    return image_tensor, mask_tensor\n",
    "\n",
    "# Example usage\n",
    "image_tensor, mask_tensor = to_tensor(image_aug, mask_aug)\n",
    "print(f\"Image tensor shape: {image_tensor.shape}, mask tensor shape: {mask_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cbb7b1",
   "metadata": {
    "id": "04cbb7b1"
   },
   "source": [
    "- `permute(2, 0, 1)` changes the order from `(H, W, C)` to `(C, H, W)` as expected by PyTorch models.\n",
    "\n",
    "- `mask` is converted to `long` (int64) because segmentation masks are class indices (not floats)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14b9d2",
   "metadata": {
    "id": "1a14b9d2"
   },
   "source": [
    "#### 7. Normalize image using ImageNet stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "LxScjVd3iPTa",
   "metadata": {
    "id": "LxScjVd3iPTa"
   },
   "outputs": [],
   "source": [
    "# Define normalization transform (ImageNet mean/std)\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "# Apply normalization\n",
    "image_normalized_tensor = normalize(image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257a359d",
   "metadata": {
    "id": "257a359d"
   },
   "source": [
    "- Normalizes each channel using ImageNet statistics (common for models like ResNet, VGG, etc.).\n",
    "\n",
    "- This helps the model train faster and more stably.\n",
    "\n",
    "- Only applied to the image; the mask is left as-is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bcb28a",
   "metadata": {
    "id": "b7bcb28a"
   },
   "source": [
    "#### 8. Convert mask to binary (subject vs background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "uMP3BrTwiQe4",
   "metadata": {
    "id": "uMP3BrTwiQe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in mask: tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "def make_binary_mask(mask_tensor):\n",
    "    # Convert any non-zero value to 1 (subject), 0 remains background\n",
    "    binary_mask = (mask_tensor > 0).long()\n",
    "    return binary_mask\n",
    "\n",
    "# Example usage\n",
    "binary_mask = make_binary_mask(mask_tensor)\n",
    "print(f\"Unique values in mask: {torch.unique(binary_mask)}\")  # Should be tensor([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba96b5df",
   "metadata": {
    "id": "ba96b5df"
   },
   "source": [
    "- Converts a multi-class mask into a binary mask:\n",
    "\n",
    "  - `0` → background\n",
    "\n",
    "  - `>0` → subject (set to 1)\n",
    "\n",
    "- Useful if the original mask has multiple object classes but the task is just “subject vs background”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d645902e",
   "metadata": {
    "id": "d645902e"
   },
   "source": [
    "#### Task: Implement the full pipeline and execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05968a73",
   "metadata": {
    "id": "05968a73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dummy image: data/images\\train_001.jpg\n",
      "Created dummy mask: data/masks\\train_001.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create dummy directories if they don't exist\n",
    "image_dir = \"data/images\"\n",
    "mask_dir  = \"data/masks\"\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "os.makedirs(mask_dir, exist_ok=True)\n",
    "\n",
    "# Create a dummy image (e.g., a simple white image)\n",
    "dummy_image = np.ones((256, 256, 3), dtype=np.uint8) * 255 # White image\n",
    "cv2.imwrite(os.path.join(image_dir, \"train_001.jpg\"), dummy_image)\n",
    "\n",
    "# Create a dummy mask (e.g., a simple circle in the center)\n",
    "dummy_mask = np.zeros((256, 256), dtype=np.uint8)\n",
    "cv2.circle(dummy_mask, (128, 128), 50, 255, -1) # White circle\n",
    "cv2.imwrite(os.path.join(mask_dir, \"train_001.png\"), dummy_mask)\n",
    "\n",
    "print(f\"Created dummy image: {os.path.join(image_dir, 'train_001.jpg')}\")\n",
    "print(f\"Created dummy mask: {os.path.join(mask_dir, 'train_001.png')}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
