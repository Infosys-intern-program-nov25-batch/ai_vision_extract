{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Se1uPLD9S7Q"
   },
   "source": [
    "### Task: Using this sample dataset, create or optimize data preprocessing pipeline using opencv and pytorch and include image mask technique in the pipeline and save the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3211,
     "status": "ok",
     "timestamp": 1764950675116,
     "user": {
      "displayName": "Ravi Chandr",
      "userId": "04022233043678125699"
     },
     "user_tz": -330
    },
    "id": "CwTRVPN88eC8",
    "outputId": "0e5c6130-13ba-4f49-d6a7-8d5533e60e18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tF86DNFZ8WwJ"
   },
   "outputs": [],
   "source": [
    "# data_path = '/content/drive/MyDrive/AI_Vision_Extract_Nov25/data (1)/train2017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "executionInfo": {
     "elapsed": 85,
     "status": "error",
     "timestamp": 1764950662659,
     "user": {
      "displayName": "Ravi Chandr",
      "userId": "04022233043678125699"
     },
     "user_tz": -330
    },
    "id": "494c9dde",
    "outputId": "b406fc19-b408-42a4-f259-0600f6497fa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of /content/drive/MyDrive/AI_Vision_Extract_Nov25/data (1)/train2017:\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/AI_Vision_Extract_Nov25/data (1)/train2017'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3665882253.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# List the contents of the data_path directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Contents of {data_path}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"- {item}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mimages_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val2017'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Common COCO image directory name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/AI_Vision_Extract_Nov25/data (1)/train2017'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List the contents of the data_path directory\n",
    "print(f\"Contents of {data_path}:\")\n",
    "for item in os.listdir(data_path):\n",
    "    print(f\"- {item}\")\n",
    "images_dir = os.path.join(data_path, 'val2017') # Common COCO image directory name\n",
    "annotations_dir = os.path.join(data_path, 'annotations') # Common COCO annotations directory name\n",
    "\n",
    "\n",
    "print(f\"\\nProposed image directory path: {images_dir}\")\n",
    "print(f\"Proposed annotations directory path: {annotations_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3ce6956"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# User provided parent directory\n",
    "parent_data_path = '/content/drive/MyDrive/AI_Vision_Extract_Nov25'\n",
    "\n",
    "print(f\"Contents of {parent_data_path}:\")\n",
    "if os.path.exists(parent_data_path):\n",
    "    for item in os.listdir(parent_data_path):\n",
    "        print(f\"- {item}\")\n",
    "else:\n",
    "    print(f\"Error: The path '{parent_data_path}' does not exist. Please double-check the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdd1a586"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Correct images_dir to point directly to data_path as images are listed there.\n",
    "images_dir = data_path\n",
    "print(f\"Updated image directory path: {images_dir}\")\n",
    "\n",
    "# Identify image files\n",
    "image_files = glob.glob(os.path.join(images_dir, '*.jpg')) # Assuming JPG images based on previous output\n",
    "print(f\"Found {len(image_files)} image files.\")\n",
    "\n",
    "if image_files:\n",
    "    sample_image_path = image_files[0]\n",
    "    print(f\"Sample image file: {sample_image_path}\")\n",
    "else:\n",
    "    sample_image_path = None\n",
    "    print(\"No image files found in the specified path.\")\n",
    "\n",
    "# Now, let's check for COCO annotation files.\n",
    "# Common COCO annotation filenames are like 'instances_val2017.json', 'captions_val2017.json', etc.\n",
    "# We'll look for any .json file in data_path or a potential 'annotations' subdirectory if it existed.\n",
    "\n",
    "# Check in data_path directly\n",
    "annotation_files_in_data_path = glob.glob(os.path.join(data_path, '*.json'))\n",
    "print(f\"Found {len(annotation_files_in_data_path)} JSON annotation files in {data_path}: {annotation_files_in_data_path}\")\n",
    "\n",
    "# Check in the *assumed* annotations_dir, even if we know it doesn't exist based on previous listing,\n",
    "# just to be thorough and inform the user.\n",
    "assumed_annotations_dir = os.path.join(data_path, 'annotations')\n",
    "annotation_files_in_assumed_dir = glob.glob(os.path.join(assumed_annotations_dir, '*.json'))\n",
    "print(f\"Found {len(annotation_files_in_assumed_dir)} JSON annotation files in {assumed_annotations_dir}: {annotation_files_in_assumed_dir}\")\n",
    "\n",
    "# Based on the output, it's highly likely that a COCO annotation file is not present\n",
    "# in the immediate data_path or a standard 'annotations' subfolder.\n",
    "if not annotation_files_in_data_path and not annotation_files_in_assumed_dir:\n",
    "    coco_annotation_file = None\n",
    "    print(\"\\nWarning: No COCO-like JSON annotation files found in the provided data_path or a standard 'annotations' subdirectory.\")\n",
    "    print(\"Please ensure the COCO annotation file (e.g., instances_val2017.json) is present or specify its correct path.\")\n",
    "else:\n",
    "    # If there are multiple, we need to pick the right one, usually 'instances_*.json' for object detection/segmentation\n",
    "    # For now, let's just take the first one if found\n",
    "    if annotation_files_in_data_path:\n",
    "        coco_annotation_file = annotation_files_in_data_path[0]\n",
    "    else:\n",
    "        coco_annotation_file = annotation_files_in_assumed_dir[0]\n",
    "    print(f\"Proposed COCO annotation file: {coco_annotation_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6ff749c"
   },
   "source": [
    "### Update `data_path` to your new image directory\n",
    "\n",
    "Edit the `data_path` variable in the cell below to point to the new Google Drive folder where your desired images are located. After updating, please run this cell and then continue executing the notebook cells from the beginning of the 'Inspect and Load Dataset' subtask onwards to apply the new images to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8c81669"
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: Replace the path below with the path to your new image directory in Google Drive.\n",
    "# Example: data_path = '/content/drive/MyDrive/MyNewImages'\n",
    "data_path = '/content/drive/MyDrive/AI_Vision_Extract_Nov25/data (1)/train2017'\n",
    "\n",
    "print(f\"Data path updated to: {data_path}\")\n",
    "\n",
    "# You would typically re-run the notebook from the 'Inspect and Load Dataset' section\n",
    "# to load images from this new path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5091479c"
   },
   "source": [
    "**Reasoning**:\n",
    "The previous steps have confirmed the location of image files and the absence of COCO annotation files. Now, following the instructions, I will load and display a sample image using `cv2` and `matplotlib.pyplot`. I will also explicitly handle the case where no COCO annotation file was found.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2801cd79"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import json # Although no JSON file was found, it's good to import as per instructions\n",
    "\n",
    "print(f\"Attempting to load sample image: {sample_image_path}\")\n",
    "\n",
    "if sample_image_path and os.path.exists(sample_image_path):\n",
    "    # Load the image using OpenCV\n",
    "    image = cv2.imread(sample_image_path)\n",
    "\n",
    "    # OpenCV loads images in BGR format, convert to RGB for matplotlib display\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the image\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.title(f\"Sample Image: {os.path.basename(sample_image_path)}\")\n",
    "    plt.axis('off') # Hide axes ticks\n",
    "    plt.show()\n",
    "    print(\"Sample image loaded and displayed successfully.\")\n",
    "else:\n",
    "    print(f\"Error: Sample image file not found at {sample_image_path}.\")\n",
    "\n",
    "# Handle COCO annotations\n",
    "if coco_annotation_file:\n",
    "    print(f\"Attempting to load COCO annotations from: {coco_annotation_file}\")\n",
    "    try:\n",
    "        with open(coco_annotation_file, 'r') as f:\n",
    "            coco_data = json.load(f)\n",
    "        print(\"COCO annotation file loaded successfully. Displaying first few keys:\")\n",
    "        # Display some high-level keys to understand structure\n",
    "        for key, value in coco_data.items():\n",
    "            print(f\"- {key}: {type(value)}\")\n",
    "            if isinstance(value, list) and len(value) > 0:\n",
    "                print(f\"  First element type: {type(value[0])}\")\n",
    "                print(f\"  Length: {len(value)}\")\n",
    "                if len(value) > 0 and isinstance(value[0], dict):\n",
    "                    print(f\"  First element keys: {list(value[0].keys())}\")\n",
    "            elif isinstance(value, dict):\n",
    "                print(f\"  Keys: {list(value.keys())}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading COCO annotation file: {e}\")\n",
    "        coco_data = None\n",
    "else:\n",
    "    print(\"\\nSkipping COCO annotation loading as no annotation file was found.\")\n",
    "    coco_data = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87349ec6"
   },
   "source": [
    "## Implement Image and Mask Loading/Generation\n",
    "\n",
    "### Subtask:\n",
    "Develop a function to load an image using OpenCV and generate its corresponding binary segmentation mask from the COCO annotations using `pycocotools`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc142f18"
   },
   "source": [
    "**Reasoning**:\n",
    "The previous steps confirmed the absence of COCO annotation files. To proceed with the subtask's objective of demonstrating the image mask technique, I will now implement a function to load an image and generate a placeholder binary mask, as instructed. This function will load the image, convert it to RGB, and create an all-white mask of the same dimensions. Finally, I will test this function and display both the image and the placeholder mask.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44a5d6f3"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def load_image_and_placeholder_mask(image_path):\n",
    "    \"\"\"\n",
    "    Loads an image using OpenCV and generates a placeholder binary mask.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The full path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - image_rgb (np.ndarray): The loaded image in RGB format.\n",
    "            - placeholder_mask (np.ndarray): A binary placeholder mask of the same dimensions.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image file not found at: {image_path}\")\n",
    "\n",
    "    # Load the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Could not load image from: {image_path}\")\n",
    "\n",
    "    # Convert image from BGR to RGB format\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Get image dimensions (height, width)\n",
    "    h, w, _ = image_rgb.shape\n",
    "\n",
    "    # Create an all-white placeholder binary mask (all pixels are 1)\n",
    "    # The mask should have the same height and width as the image, and be of type uint8\n",
    "    placeholder_mask = np.ones((h, w), dtype=np.uint8) * 255 # All white mask\n",
    "\n",
    "    return image_rgb, placeholder_mask\n",
    "\n",
    "# Test the function with a few images from image_files\n",
    "print(f\"Testing load_image_and_placeholder_mask with a few sample images:\")\n",
    "\n",
    "num_test_images = min(3, len(image_files)) # Test with up to 3 different images\n",
    "\n",
    "plt.figure(figsize=(12, 6 * num_test_images)) # Adjust figure size for multiple rows\n",
    "\n",
    "for i in range(num_test_images):\n",
    "    current_image_path = image_files[i]\n",
    "    print(f\"\\n--- Testing with: {current_image_path} ---\")\n",
    "    try:\n",
    "        loaded_image_rgb_test, generated_mask_test = load_image_and_placeholder_mask(current_image_path)\n",
    "\n",
    "        # Display the loaded image and the generated placeholder mask\n",
    "        plt.subplot(num_test_images, 2, 2 * i + 1)\n",
    "        plt.imshow(loaded_image_rgb_test)\n",
    "        plt.title(f\"Loaded Image: {os.path.basename(current_image_path)}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(num_test_images, 2, 2 * i + 2)\n",
    "        plt.imshow(generated_mask_test, cmap='gray')\n",
    "        plt.title(\"Placeholder Mask\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error during function test for {current_image_path}: {e}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Function tested successfully with multiple images.\")\n",
    "\n",
    "# Re-run for the first image to retain original `loaded_image_rgb` and `generated_mask`\n",
    "# variables for consistency with subsequent cells in the notebook.\n",
    "loaded_image_rgb, generated_mask = load_image_and_placeholder_mask(image_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ae00321"
   },
   "source": [
    "## Define Preprocessing Transforms (OpenCV & PyTorch)\n",
    "\n",
    "### Subtask:\n",
    "Define a set of preprocessing transformations including resizing images and masks, normalization for images, and conversion to PyTorch tensors. Ensure that mask resizing uses appropriate interpolation (e.g., nearest-neighbor) to preserve boundaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d70121d"
   },
   "source": [
    "**Reasoning**:\n",
    "I need to define a custom preprocessing transform class that resizes images and masks, converts them to PyTorch tensors, and normalizes the image. I will follow the instructions to implement the `ImageMaskTransforms` class, instantiate it, and then test it with the previously loaded image and generated mask, printing their shapes and data types to verify the transformations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a24f1b3e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 2. Define a target size for resizing images and masks\n",
    "target_size = (256, 256)\n",
    "\n",
    "# 3. Create a custom transform class (e.g., ImageMaskTransforms)\n",
    "class ImageMaskTransforms:\n",
    "    def __init__(self, target_size, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        self.target_size = target_size\n",
    "        self.normalize = T.Normalize(mean=mean, std=std)\n",
    "        self.to_tensor = T.ToTensor() # Converts PIL Image or numpy.ndarray (H x W x C) to a float tensor (C x H x W) and scales pixel values to [0.0, 1.0]\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        # a. Resize the input image using cv2.resize()\n",
    "        # cv2 expects (width, height) for target size\n",
    "        resized_image = cv2.resize(image, self.target_size, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        # b. Resize the input mask using cv2.resize() with cv2.INTER_NEAREST\n",
    "        # Ensure mask remains single channel and binary (0 or 1)\n",
    "        resized_mask = cv2.resize(mask, self.target_size, interpolation=cv2.INTER_NEAREST)\n",
    "        # Ensure mask values are binary (0 or 1) after resizing if they were 0 or 255\n",
    "        resized_mask = (resized_mask > 0).astype(np.uint8) # Convert to 0/1 if it was 0/255\n",
    "\n",
    "        # c. Convert both the resized image and mask into PyTorch tensors\n",
    "        image_tensor = self.to_tensor(resized_image) # Already scales to [0, 1]\n",
    "        mask_tensor = torch.from_numpy(resized_mask).float() # Convert to float tensor\n",
    "\n",
    "        # Masks typically have shape (H, W), add a channel dimension to make it (C, H, W) for consistency if needed,\n",
    "        # but for segmentation masks, (H, W) or (1, H, W) is common.\n",
    "        # Let's keep it (H, W) for now, as it's a binary mask.\n",
    "        # If it were a multi-class mask, we might want (1, H, W).\n",
    "        # For now, ensure it's (H, W) or (B, H, W) for batch, let's keep (H, W) as output.\n",
    "        if mask_tensor.dim() == 2: # if it's (H, W)\n",
    "            mask_tensor = mask_tensor.unsqueeze(0) # Make it (1, H, W)\n",
    "\n",
    "        # d. Apply normalization to the image tensor\n",
    "        normalized_image_tensor = self.normalize(image_tensor)\n",
    "\n",
    "        # e. Return the transformed image tensor and mask tensor\n",
    "        return normalized_image_tensor, mask_tensor\n",
    "\n",
    "# 5. Instantiate your custom ImageMaskTransforms class\n",
    "transforms = ImageMaskTransforms(target_size=target_size)\n",
    "\n",
    "# 6. Test the transformations with loaded_image_rgb and generated_mask\n",
    "print(f\"Original image shape: {loaded_image_rgb.shape}, dtype: {loaded_image_rgb.dtype}\")\n",
    "print(f\"Original mask shape: {generated_mask.shape}, dtype: {generated_mask.dtype}\")\n",
    "\n",
    "transformed_image, transformed_mask = transforms(loaded_image_rgb, generated_mask)\n",
    "\n",
    "print(f\"\\nTransformed image tensor shape: {transformed_image.shape}, dtype: {transformed_image.dtype}\")\n",
    "print(f\"Transformed mask tensor shape: {transformed_mask.shape}, dtype: {transformed_mask.dtype}\")\n",
    "print(f\"Transformed image tensor min/max: {transformed_image.min():.4f}/{transformed_image.max():.4f}\")\n",
    "print(f\"Transformed mask tensor min/max: {transformed_mask.min():.4f}/{transformed_mask.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85570d9f"
   },
   "source": [
    "## Build PyTorch Dataset and DataLoader\n",
    "\n",
    "### Subtask:\n",
    "Create a custom PyTorch `Dataset` class that integrates the image/mask loading and preprocessing steps. Then, instantiate a `DataLoader` to efficiently handle batching and loading of the processed data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41c34883"
   },
   "source": [
    "**Reasoning**:\n",
    "I need to implement the `ImageMaskDataset` class, its `__len__` and `__getitem__` methods, instantiate it with the existing `image_files` and `transforms`, and then create a `DataLoader`. Finally, I will verify the output by fetching a batch and printing the shapes of the image and mask tensors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a22c1814"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Define a custom PyTorch Dataset class named ImageMaskDataset\n",
    "class ImageMaskDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    # 2. Implement the __len__ method\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    # 3. Implement the __getitem__ method\n",
    "    def __getitem__(self, idx):\n",
    "        # a. Retrieve the image path for the given index.\n",
    "        image_path = self.image_paths[idx]\n",
    "\n",
    "        # b. Use the previously defined load_image_and_placeholder_mask function.\n",
    "        # This function loads the image in RGB and returns an all-white placeholder mask.\n",
    "        image_rgb, placeholder_mask = load_image_and_placeholder_mask(image_path)\n",
    "\n",
    "        # c. Apply the transform object to the loaded image and mask.\n",
    "        if self.transform:\n",
    "            transformed_image, transformed_mask = self.transform(image_rgb, placeholder_mask)\n",
    "        else:\n",
    "            # If no transform is provided, convert to tensor manually (assuming no normalization)\n",
    "            transformed_image = T.ToTensor()(image_rgb)\n",
    "            transformed_mask = torch.from_numpy(placeholder_mask).float().unsqueeze(0) # Add channel dim\n",
    "\n",
    "        # d. Return the transformed image and mask tensors.\n",
    "        return transformed_image, transformed_mask\n",
    "\n",
    "# 4. Create an instance of your ImageMaskDataset\n",
    "# `image_files` and `transforms` were defined in previous steps.\n",
    "# Ensure `load_image_and_placeholder_mask` is accessible (it was defined globally earlier).\n",
    "image_mask_dataset = ImageMaskDataset(image_paths=image_files, transform=transforms)\n",
    "\n",
    "print(f\"Number of items in the dataset: {len(image_mask_dataset)}\")\n",
    "\n",
    "# 5. Instantiate a torch.utils.data.DataLoader\n",
    "batch_size = 4 # Or 8, as suggested\n",
    "# num_workers = 2 # Set to 0 for simpler debugging, or >0 for faster loading in production\n",
    "\n",
    "# Check if running in Colab to adjust num_workers\n",
    "# Using 0 workers for simplicity to avoid issues with multiprocessing on Colab/Jupyter\n",
    "# Unless specifically required and environment is stable, 0 is often safer for debugging.\n",
    "# num_workers = os.cpu_count() // 2 if os.cpu_count() else 0 # A safer way to set num_workers\n",
    "num_workers = 0 # Explicitly setting to 0 for robust execution in varied environments\n",
    "\n",
    "data_loader = DataLoader(image_mask_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "print(f\"DataLoader created with batch_size={batch_size} and num_workers={num_workers}\")\n",
    "\n",
    "# 6. Iterate once through the DataLoader to fetch a single batch and print shapes\n",
    "try:\n",
    "    for images_batch, masks_batch in data_loader:\n",
    "        print(f\"\\nShape of image batch: {images_batch.shape}\")\n",
    "        print(f\"Shape of mask batch: {masks_batch.shape}\")\n",
    "        break # Only take one batch for verification\n",
    "    print(\"DataLoader batch successfully fetched and shapes verified.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching batch from DataLoader: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "928442a4"
   },
   "source": [
    "## Test Pipeline and Save Output\n",
    "\n",
    "### Subtask:\n",
    "Test the constructed `DataLoader` by fetching a batch of processed images and masks. Visualize a few samples to verify the correctness of the preprocessing and masking. Save a few processed image and mask pairs to a specified output directory to demonstrate the pipeline's output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "909c1f7d"
   },
   "source": [
    "**Reasoning**:\n",
    "I need to implement the instructions to test the pipeline by fetching a batch from the `DataLoader`, visualize a few samples, and save them to a new output directory. This involves denormalization, tensor-to-numpy conversions, and file operations using `cv2` and `matplotlib`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b536654a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# 1. Fetch a batch of processed images and masks from the data_loader\n",
    "# Iterate once to get the first batch\n",
    "images_batch, masks_batch = next(iter(data_loader))\n",
    "\n",
    "print(f\"Fetched batch - Images shape: {images_batch.shape}, Masks shape: {masks_batch.shape}\")\n",
    "\n",
    "# 2. Create an output directory named 'processed_output'\n",
    "output_dir = 'processed_output'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created output directory: {output_dir}\")\n",
    "else:\n",
    "    print(f\"Output directory '{output_dir}' already exists.\")\n",
    "\n",
    "# Get mean and std used for normalization from the transforms object\n",
    "# The mean and std are stored in the normalize attribute of the transforms object\n",
    "mean = torch.tensor(transforms.normalize.mean).view(3, 1, 1) # Reshape for broadcasting (C, 1, 1)\n",
    "std = torch.tensor(transforms.normalize.std).view(3, 1, 1)   # Reshape for broadcasting (C, 1, 1)\n",
    "\n",
    "# 3. Iterate through a few samples (e.g., 2-3) in the fetched batch\n",
    "num_samples_to_show = min(3, images_batch.shape[0]) # Show up to 3 samples or less if batch is smaller\n",
    "\n",
    "plt.figure(figsize=(15, 5 * num_samples_to_show))\n",
    "\n",
    "for i in range(num_samples_to_show):\n",
    "    # Get individual image and mask from batch\n",
    "    image_tensor = images_batch[i]\n",
    "    mask_tensor = masks_batch[i]\n",
    "\n",
    "    # a. Denormalize the image tensor\n",
    "    # (C, H, W) -> (C, H, W)\n",
    "    denormalized_image = image_tensor * std + mean\n",
    "    # Clamp values to [0, 1] in case of slight over/underflow due to floating point operations\n",
    "    denormalized_image = torch.clamp(denormalized_image, 0, 1)\n",
    "\n",
    "    # b. Convert denormalized image tensor to NumPy array (H, W, C) and scale to [0, 255]\n",
    "    # Move channel to last dimension and convert to uint8\n",
    "    # (C, H, W) -> (H, W, C)\n",
    "    image_np = (denormalized_image.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "\n",
    "    # c. Convert mask tensor to NumPy array, scale to [0, 255] and change dtype to uint8\n",
    "    # Mask tensor is (1, H, W), remove channel dimension, scale to [0, 255] if values are 0/1\n",
    "    mask_np = (mask_tensor.squeeze(0).numpy() * 255).astype(np.uint8)\n",
    "\n",
    "    # d. Display the original (denormalized) image and its corresponding mask side-by-side\n",
    "    plt.subplot(num_samples_to_show, 2, 2 * i + 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.title(f\"Processed Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(num_samples_to_show, 2, 2 * i + 2)\n",
    "    plt.imshow(mask_np, cmap='gray') # Use gray colormap for binary masks\n",
    "    plt.title(f\"Processed Mask {i+1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # e. Save the processed image and mask to the 'processed_output' directory\n",
    "    image_filename = os.path.join(output_dir, f\"processed_image_{i+1}.jpg\")\n",
    "    mask_filename = os.path.join(output_dir, f\"processed_mask_{i+1}.png\")\n",
    "\n",
    "    # For saving with cv2, convert RGB (image_np) to BGR\n",
    "    cv2.imwrite(image_filename, cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(mask_filename, mask_np) # Masks are typically grayscale, no BGR conversion needed\n",
    "\n",
    "    print(f\"Saved {image_filename} and {mask_filename}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Visualization and saving complete for {num_samples_to_show} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "215632bf"
   },
   "source": [
    "## Final Task\n",
    "\n",
    "### Subtask:\n",
    "Summarize the created data preprocessing pipeline, highlighting the use of OpenCV for image manipulation, PyTorch for tensor conversion, and the image mask technique, and discuss how the output is saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa9d9af4"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "### Q&A\n",
    "The created data preprocessing pipeline effectively handles image manipulation using OpenCV, converts data to PyTorch tensors, incorporates an image mask technique (though a placeholder in this instance), and saves the processed output as follows:\n",
    "\n",
    "1.  **OpenCV for Image Manipulation**: OpenCV (`cv2`) was used to load images in BGR format, which were then converted to RGB. It was also utilized for resizing both images and masks to a target size of `(256, 256)`. For images, `cv2.INTER_LINEAR` interpolation was applied, and for masks, `cv2.INTER_NEAREST` was used to preserve sharp boundaries.\n",
    "2.  **PyTorch for Tensor Conversion**: Images and masks were converted into PyTorch tensors using `torchvision.transforms.ToTensor()` for images (which also scales pixel values to \\[0.0, 1.0]) and `torch.from_numpy()` for masks. Image tensors were further normalized using predefined mean and standard deviation values. Masks were given an additional channel dimension, resulting in shapes like `torch.Size([3, 256, 256])` for images and `torch.Size([1, 256, 256])` for masks.\n",
    "3.  **Image Mask Technique**: Due to the absence of COCO annotation files in the provided dataset, a placeholder binary mask (an all-white mask with values of 255, later converted to 1.0 in tensor form) was generated for each image. This demonstrated the pipeline's capability to handle masks, even if they were not derived from actual annotations in this specific execution.\n",
    "4.  **Output Saving**: After processing and visualization, the pipeline saved a few samples of the processed images and their corresponding masks. Images were denormalized, converted back to NumPy arrays, and saved as JPEG files (e.g., `processed_image_1.jpg`) in RGB format (converted to BGR for `cv2.imwrite`). Masks were converted to NumPy arrays and saved as PNG files (e.g., `processed_mask_1.png`) in grayscale within a newly created `processed_output` directory.\n",
    "\n",
    "### Data Analysis Key Findings\n",
    "*   The `COCO2017_SAMPLE` dataset contained image files (e.g., `image_0.jpg`) directly in the root directory `/content/drive/MyDrive/AI_Vision_Extract_Nov25/data (1)/COCO2017_SAMPLE`, not in typical `val2017` subdirectories.\n",
    "*   No COCO-like JSON annotation files were found in the dataset, leading to the use of a placeholder all-white binary mask for demonstration.\n",
    "*   A custom `ImageMaskTransforms` class successfully implemented resizing (images via `cv2.INTER_LINEAR`, masks via `cv2.INTER_NEAREST` to a target size of `(256, 256)`), conversion to PyTorch tensors (`torch.Size([3, 256, 256])` for images, `torch.Size([1, 256, 256])` for masks), and image normalization.\n",
    "*   A custom `ImageMaskDataset` and `DataLoader` were successfully created and verified, efficiently handling batching of processed images and placeholder masks, with batch shapes `torch.Size([4, 3, 256, 256])` for images and `torch.Size([4, 1, 256, 256])` for masks.\n",
    "*   The pipeline successfully saved processed images as `.jpg` and masks as `.png` to a `processed_output` directory, demonstrating its ability to persist the transformed data.\n",
    "\n",
    "### Insights or Next Steps\n",
    "*   To make the pipeline fully functional for segmentation tasks, the primary next step is to obtain or generate actual COCO-formatted annotation files for the `COCO2017_SAMPLE` dataset. This would allow the `load_image_and_placeholder_mask` function to be extended to load real segmentation masks using `pycocotools`.\n",
    "*   Consider adding more advanced data augmentation techniques (e.g., random flips, rotations, color jitter) within the `ImageMaskTransforms` class to improve model robustness, ensuring masks are transformed consistently with images.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
